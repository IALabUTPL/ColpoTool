{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d496ae-286f-4ede-bb90-b3a5eb331461",
   "metadata": {},
   "source": [
    "# Cervical Lesion Classification with Detectron2\r\n",
    "\r\n",
    "## Description\r\n",
    "\r\n",
    "This notebook aims to develop a deep learning model using the Mask R-CNN architecture from Detectron2 for the **classification and detection of cervical lesions** in biomedical images. The process begins with annotations previously generated in **COCO JSON** or **VGG VIA JSON** format. It includes the unification and conversion of annotations, dataset registration in Detectron2, sample visualization, model training, and performance evaluation.\r\n",
    "\r\n",
    "Detectron2 provides a robust and highly optimized framework for instance segmentation and object detection, and its application in this context seeks to automate and improve the precision of colposcopic diagnosis.\r\n",
    "\r\n",
    "## Authors\r\n",
    "\r\n",
    "- **Ramiro Israel Vivanco Gual√°n**, MSc  \r\n",
    "  Department of Chemistry and Exact Sciences, Universidad T√©cnica Particular de Loja (UTPL), Ecuador\r\n",
    "\r\n",
    "- **Yuliana Jim√©nez-Gaona**, MSc  \r\n",
    "  Department of Chemistry and Exact Sciences, UTPL, Ecuador\r\n",
    "\r\n",
    "- **Bernardo Vega-Crespo**, MD, PhD  \r\n",
    "  Faculty of Medical Sciences, Universidad de Cuenca, Ecuador\r\n",
    "\r\n",
    "- **Ver√≥nica Mu√±oz**, MD  \r\n",
    "  Faculty of Medical Sciences, Universidad de Cuenca, Ecuador\r\n",
    "\r\n",
    "- **Veronique Verhoeven**, MD, PhD  \r\n",
    "  University of Antwerp, Belgium\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Objectives\r\n",
    "\r\n",
    "- Unify annotations from various sources (COCO, VGG VIA) into a single standard file.\r\n",
    "- Register custom datasets in Detectron2 for training and validation.\r\n",
    "- Train a Mask R-CNN model from the `model_zoo` for lesion classification and segmentation.\r\n",
    "- Evaluate the model's performance in terms of class-wise detection and segmentation accuracy.\r\n",
    "- Export segmentation results, predictions, and evaluation metrics in both visual and numerical formats.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Dataset\r\n",
    "\r\n",
    "- **Original formats**: COCO JSON, VGG VIA JSON.\r\n",
    "- **Expected labels**: `demy`, `isch`, among other lesion-related classes.\r\n",
    "- **Source**: Biomedical cervix images from clinical studies (e.g., Intel dataset, KAIME).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Tools and Libraries\r\n",
    "\r\n",
    "- Jupyter Notebook / Google Colab\r\n",
    "- Detectron2 (Facebook AI Research)\r\n",
    "- OpenCV, Matplotlib, Pandas\r\n",
    "- PyTorch (CUDA enabled)\r\n",
    "- makesense.ai / VGG VIA (for annotation purposes)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d7280b-fa12-43e3-ac0f-926cab151a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.1.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
      "Collecting triton==2.1.0 (from torch==2.1.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.2)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.1.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Installing collected packages: triton, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.0\n",
      "    Uninstalling triton-3.3.0:\n",
      "      Successfully uninstalled triton-3.3.0\n",
      "Successfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90981c8-5926-45ed-a9bc-297d89da483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# File and Data Handling\n",
    "# =======================\n",
    "import os                              # File system operations\n",
    "import json                            # JSON file manipulation\n",
    "import uuid                            # Unique identifiers\n",
    "from glob import glob                  # File search using patterns\n",
    "from pathlib import Path               # Object-oriented filesystem paths\n",
    "import warnings                        # Suppress warning messages\n",
    "import h5py                            # HDF5 file I/O for saving models\n",
    "import joblib                          # For saving trained models\n",
    "import random                          # Random number generation\n",
    "from collections import Counter        # Frequency counting\n",
    "\n",
    "# =====================\n",
    "# Numerical Processing\n",
    "# =====================\n",
    "import numpy as np                     # Numerical array operations\n",
    "import pandas as pd                    # DataFrame structures and CSV handling\n",
    "\n",
    "# ====================\n",
    "# Image Processing\n",
    "# ====================\n",
    "import cv2                             # OpenCV for image handling\n",
    "from PIL import Image                  # PIL for image manipulation\n",
    "\n",
    "# ===================\n",
    "# Data Visualization\n",
    "# ===================\n",
    "import matplotlib.pyplot as plt        # Plotting and data visualization\n",
    "import seaborn as sns                  # Advanced plotting (heatmaps, distributions)\n",
    "\n",
    "# ============================\n",
    "# Machine Learning Utilities\n",
    "# ============================\n",
    "from sklearn.model_selection import train_test_split  # Train/test split\n",
    "\n",
    "# ============================\n",
    "# Deep Learning (TensorFlow)\n",
    "# ============================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model             # Model architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import MeanIoU           # Segmentation metric\n",
    "\n",
    "# ===================\n",
    "# Detectron2 Framework\n",
    "# ===================\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    ")\n",
    "from detectron2.data.datasets import (\n",
    "    register_coco_instances,\n",
    "    coco,\n",
    "    load_coco_json,\n",
    ")\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.evaluation import (\n",
    "    COCOEvaluator,\n",
    "    inference_on_dataset,\n",
    ")\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# ===================\n",
    "# Memory Management\n",
    "# ===================\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ===================\n",
    "# Progress Monitoring\n",
    "# ===================\n",
    "from tqdm import tqdm                  # Progress bar for loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd29b3-b39e-4b91-b076-04ded69d1d14",
   "metadata": {},
   "source": [
    "## Actual Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af675af-40d6-4d0d-ba51-1b3fa1995335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real paths\n",
    "BASE_DIR = Path.cwd()\n",
    "INTEL_DIR = BASE_DIR / \"ColpoToolLab\" / \"intel\"\n",
    "JSON_DIR = BASE_DIR / \"ColpoToolLab\" / \"detectron_data\"\n",
    "\n",
    "print(f\"üìÇ INTEL_DIR: {INTEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad1d85-2586-4fd1-bd53-32d0e6b432f2",
   "metadata": {},
   "source": [
    "## JSON Files to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcf8b5-4e8e-4600-8f87-b408f8705d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ JSONs to use\n",
    "json_paths = [\n",
    "    JSON_DIR / \"colpo_2_coco.json\",\n",
    "    JSON_DIR / \"coco_labels_32_aw_m_p_av_2025-04-10-07-28-25.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25769e-a1d2-4703-a280-28a9fa54f3fe",
   "metadata": {},
   "source": [
    "## List Real Images in Intel Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18a047-d288-4c2b-aa10-cb27ba26ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: List real images in Intel folder\n",
    "imagenes_intel = {img.name for img in INTEL_DIR.glob(\"*.jpg\")}\n",
    "print(f\"üñºÔ∏è Total .jpg images in Intel: {len(imagenes_intel)}\")\n",
    "print(f\"üîç Examples: {list(imagenes_intel)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed6e1c-e065-483c-9332-ae2fdc0764e9",
   "metadata": {},
   "source": [
    "## Load JSONs and Filter Valid Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8821d8-e90d-405d-a95e-daf3c4fc1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load JSONs and filter valid images\n",
    "imagenes_validas = {}\n",
    "anotaciones_validas = []\n",
    "\n",
    "for json_file in json_paths:\n",
    "    print(f\"\\nüì¶ Processing: {json_file.name}\")\n",
    "    if not json_file.exists():\n",
    "        print(f\"‚ùå File not found: {json_file}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images_json = data.get(\"images\", [])\n",
    "    annotations_json = data.get(\"annotations\", [])\n",
    "\n",
    "    print(f\"üì∑ Images in JSON: {len(images_json)} | üß© Annotations: {len(annotations_json)}\")\n",
    "\n",
    "    # Validate images that physically exist\n",
    "    for img in images_json:\n",
    "        nombre = Path(img[\"file_name\"]).name\n",
    "        ruta_imagen = INTEL_DIR / nombre\n",
    "\n",
    "        if ruta_imagen.exists():\n",
    "            imagenes_validas[img[\"id\"]] = nombre\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing image on disk: {nombre}\")\n",
    "\n",
    "    # Validate annotations whose image exists\n",
    "    for ann in annotations_json:\n",
    "        if ann[\"image_id\"] in imagenes_validas:\n",
    "            nombre_img = imagenes_validas[ann[\"image_id\"]]\n",
    "            ruta_img = INTEL_DIR / nombre_img\n",
    "\n",
    "            if ruta_img.exists():  # Re-confirmation (double check)\n",
    "                segmentacion = ann.get(\"segmentation\", [])\n",
    "                if isinstance(segmentacion, list) and len(segmentacion) > 0 and len(segmentacion[0]) >= 6:\n",
    "                    anotaciones_validas.append({\n",
    "                        \"nombre\": nombre_img,\n",
    "                        \"categoria_id\": ann[\"category_id\"],\n",
    "                        \"segmentacion\": segmentacion\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Invalid annotation (empty or short segmentation): {nombre_img}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0046-b457-4018-96cc-474ea31c53e6",
   "metadata": {},
   "source": [
    "## Count by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec3831-35fe-4cbd-b057-a9742b6b64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 3: Count by category\n",
    "conteo = Counter(ann[\"categoria_id\"] for ann in anotaciones_validas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9b45f-9b0a-4032-8f6d-4f0048ccbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Results\n",
    "print(f\"\\n‚úÖ Total valid images (with at least one valid annotation): {len(set(ann['nombre'] for ann in anotaciones_validas))}\")\n",
    "print(f\"‚úÖ Total valid annotations: {len(anotaciones_validas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d517c1-5c57-4662-9557-b76542fdf6d2",
   "metadata": {},
   "source": [
    "## Split into Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840440c-6b78-4193-91f2-6b11ea6f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Extra step: split into train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get unique image names\n",
    "imagenes_unicas = sorted(list(set(ann[\"nombre\"] for ann in anotaciones_validas)))\n",
    "\n",
    "# Split into 80% train, 20% val\n",
    "train_imgs, val_imgs = train_test_split(imagenes_unicas, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assign each annotation to its set\n",
    "anotaciones_train = [a for a in anotaciones_validas if a[\"nombre\"] in train_imgs]\n",
    "anotaciones_val = [a for a in anotaciones_validas if a[\"nombre\"] in val_imgs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa299699-1325-4355-ba11-1e495704896c",
   "metadata": {},
   "source": [
    "## Class Count & Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a63d3-b89b-4490-aa86-6c51a147c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Class count\n",
    "print(\"\\nüìä Count per class:\")\n",
    "for clase, cantidad in conteo.items():\n",
    "    print(f\" - Class {clase}: {cantidad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e28ad-1ad0-4980-b4a9-e736cba41365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Sample\n",
    "print(\"\\nüß™ Sample of 5 annotations:\")\n",
    "for ann in anotaciones_validas[:5]:\n",
    "    print(f\" - Image: {ann['nombre']}, Class: {ann['categoria_id']}, Segment: {str(ann['segmentacion'])[:40]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5dce1-a1a0-4136-b2a0-80d794d9ebcc",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784947a-4c6c-4992-a092-fc29e704a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Visualization\n",
    "if not conteo:\n",
    "    print(\"‚ö†Ô∏è No data to plot.\")\n",
    "else:\n",
    "    clases = list(conteo.keys())\n",
    "    cantidades = list(conteo.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(clases, cantidades, color='cornflowerblue', edgecolor='black')\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Number of Annotations\")\n",
    "    plt.title(\"Class Distribution in Valid Annotations\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22046ef-61e6-40ba-8a66-78856925b238",
   "metadata": {},
   "source": [
    "## Initialization and Data Formatting for Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cc928-9d3d-4437-b972-27dacdb1c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Initialization\n",
    "final_images = []\n",
    "final_annotations = []\n",
    "img_id_map = {}\n",
    "img_id_counter = 1\n",
    "ann_id_counter = 1\n",
    "\n",
    "for ann in anotaciones_validas:\n",
    "    nombre = ann[\"nombre\"]\n",
    "    segmentacion = ann[\"segmentacion\"]\n",
    "\n",
    "    # ‚ùå Skip empty or invalid annotations\n",
    "    if not isinstance(segmentacion, list) or len(segmentacion) == 0 or len(segmentacion[0]) < 6:\n",
    "        print(f\"‚ö†Ô∏è Invalid annotation skipped: {nombre}\")\n",
    "        continue\n",
    "\n",
    "    # üñºÔ∏è Process image if not already registered\n",
    "    if nombre not in img_id_map:\n",
    "        img_path = str(INTEL_DIR / nombre)\n",
    "\n",
    "        imagen = cv2.imread(img_path)\n",
    "        if imagen is None:\n",
    "            print(f\"‚ö†Ô∏è Image not found or corrupted: {nombre}\")\n",
    "            continue\n",
    "        height, width = imagen.shape[:2]  # OpenCV gives (height, width)\n",
    "\n",
    "        final_images.append({\n",
    "            \"id\": img_id_counter,\n",
    "            \"file_name\": nombre,\n",
    "            \"height\": height,\n",
    "            \"width\": width\n",
    "        })\n",
    "        img_id_map[nombre] = img_id_counter\n",
    "        img_id_counter += 1\n",
    "\n",
    "    # üì¶ Calculate bbox and area\n",
    "    puntos = segmentacion[0]\n",
    "    x_coords = puntos[0::2]\n",
    "    y_coords = puntos[1::2]\n",
    "    if not x_coords or not y_coords:\n",
    "        print(f\"‚ö†Ô∏è Empty coordinates, skipped: {nombre}\")\n",
    "        continue\n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "    bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "    area = bbox[2] * bbox[3]\n",
    "\n",
    "    final_annotations.append({\n",
    "        \"id\": ann_id_counter,\n",
    "        \"image_id\": img_id_map[nombre],\n",
    "        \"category_id\": ann[\"categoria_id\"],\n",
    "        \"segmentation\": segmentacion,\n",
    "        \"iscrowd\": 0,\n",
    "        \"bbox\": bbox,\n",
    "        \"area\": area\n",
    "    })\n",
    "    ann_id_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d77c61-a05e-4694-aa80-b25ed08364ae",
   "metadata": {},
   "source": [
    "## Define Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559f192-1120-4064-89d5-e60ba7afe878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Categories\n",
    "final_categories = [\n",
    "    {\"id\": 1, \"name\": \"aw\"},\n",
    "    {\"id\": 2, \"name\": \"m\"},\n",
    "    {\"id\": 3, \"name\": \"p\"},\n",
    "    {\"id\": 4, \"name\": \"av\"},\n",
    "    {\"id\": 5, \"name\": \"y\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6b01d-f12c-4e90-b69c-e77494b756a5",
   "metadata": {},
   "source": [
    "## Dataset Construction for Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc0001-9490-452c-a735-538270d79bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_colpo_dicts_train():\n",
    "    return construir_dataset(anotaciones_train)\n",
    "\n",
    "def get_colpo_dicts_val():\n",
    "    return construir_dataset(anotaciones_val)\n",
    "\n",
    "def construir_dataset(anotaciones_subset):\n",
    "    dataset_dicts = []\n",
    "    nombres_procesados = set()\n",
    "    img_id_counter = 0  # Incremental counter for consistent IDs\n",
    "\n",
    "    for ann in anotaciones_subset:\n",
    "        nombre = ann[\"nombre\"]\n",
    "        img_path = INTEL_DIR / nombre\n",
    "        if not img_path.exists() or nombre in nombres_procesados:\n",
    "            continue\n",
    "\n",
    "        imagen = cv2.imread(str(img_path))\n",
    "        if imagen is None:\n",
    "            continue\n",
    "\n",
    "        height, width = imagen.shape[:2]\n",
    "        record = {\n",
    "            \"file_name\": str(img_path),\n",
    "            \"image_id\": img_id_counter,  # Consistent ID, not variable like hash()\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "\n",
    "        for sub_ann in [a for a in anotaciones_subset if a[\"nombre\"] == nombre]:\n",
    "            puntos = sub_ann[\"segmentacion\"][0]\n",
    "\n",
    "            # Validate out-of-range points\n",
    "            if max(puntos) > 1.5 * max(width, height):\n",
    "                print(f\"‚ö†Ô∏è Out-of-scale coordinates in {nombre} ‚Üí max point: {max(puntos)} | Size: {width}x{height}\")\n",
    "\n",
    "            x_coords = puntos[0::2]\n",
    "            y_coords = puntos[1::2]\n",
    "            bbox = [min(x_coords), min(y_coords), max(x_coords) - min(x_coords), max(y_coords) - min(y_coords)]\n",
    "\n",
    "            record[\"annotations\"].append({\n",
    "                \"bbox\": bbox,\n",
    "                \"bbox_mode\": 0,  # BoxMode.XYWH_ABS\n",
    "                \"segmentation\": [puntos],\n",
    "                \"category_id\": sub_ann[\"categoria_id\"] - 1,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "\n",
    "        dataset_dicts.append(record)\n",
    "        nombres_procesados.add(nombre)\n",
    "        img_id_counter += 1  # Increment counter per registered image\n",
    "\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354d965-c87b-4a5f-b737-4d5da5618adc",
   "metadata": {},
   "source": [
    "## Data Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e6d90-dd09-4150-ace4-23c32add635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîé Checking malformed segmentations...\")\n",
    "\n",
    "errores_seg = 0\n",
    "for d in get_colpo_dicts_train():\n",
    "    for ann in d[\"annotations\"]:\n",
    "        if len(ann[\"segmentation\"][0]) < 6:\n",
    "            print(f\"‚ùå Invalid segmentation in {d['file_name']} with category {ann['category_id']}\")\n",
    "            errores_seg += 1\n",
    "\n",
    "print(f\"‚úÖ Verification complete. Total invalid segmentations: {errores_seg}\")\n",
    "\n",
    "print(\"üîé Checking category IDs...\")\n",
    "ids = [cat[\"id\"] for cat in final_categories]\n",
    "if sorted(ids) != list(range(len(final_categories))):\n",
    "    print(\"‚ùå Category IDs are not sequential from 0. This may cause errors.\")\n",
    "else:\n",
    "    print(\"‚úÖ Category IDs are correctly defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb710de-3626-4a76-a6cb-a2cdf74791ab",
   "metadata": {},
   "source": [
    "## Registering Datasets in Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487bb8c-aec8-403a-a1d7-76a61b60d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# üîÅ Remove if already registered\n",
    "for dname in [\"colpo_train\", \"colpo_val\"]:\n",
    "    if dname in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(dname)\n",
    "\n",
    "# ‚úÖ Re-register\n",
    "DatasetCatalog.register(\"colpo_train\", get_colpo_dicts_train)\n",
    "DatasetCatalog.register(\"colpo_val\", get_colpo_dicts_val)\n",
    "\n",
    "MetadataCatalog.get(\"colpo_train\").set(thing_classes=[c[\"name\"] for c in final_categories])\n",
    "MetadataCatalog.get(\"colpo_val\").set(thing_classes=[c[\"name\"] for c in final_categories])\n",
    "\n",
    "print(\"‚úÖ Datasets registered: colpo_train and colpo_val.\")\n",
    "\n",
    "MetadataCatalog.get(\"colpo_train\").set(thing_classes=[cat[\"name\"] for cat in final_categories])\n",
    "print(\"‚úÖ Dataset in memory registered as 'colpo_train'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35441f-1ad6-4e7d-8e67-63378b0c9601",
   "metadata": {},
   "source": [
    "## Retrieve a Sample from the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd7b25-4363-454e-8374-5bb1c73e5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Retrieve a sample from the training dataset\n",
    "muestra = get_colpo_dicts_train()[0]\n",
    "print(f\"üîé Image: {muestra['file_name']}\")\n",
    "print(f\"‚úÖ Width: {muestra['width']} | Height: {muestra['height']}\")\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.open(muestra[\"file_name\"])\n",
    "print(f\"üßæ Actual size: {img.size}\")  # Should match width and height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed816bf1-f852-4248-be4e-8c5373a22f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Re-register datasets (in case you need to force cleanup or reload)\n",
    "DatasetCatalog.remove(\"colpo_train\")\n",
    "DatasetCatalog.remove(\"colpo_val\")\n",
    "\n",
    "DatasetCatalog.register(\"colpo_train\", get_colpo_dicts_train)\n",
    "DatasetCatalog.register(\"colpo_val\", get_colpo_dicts_val)\n",
    "\n",
    "MetadataCatalog.get(\"colpo_train\").set(thing_classes=[cat[\"name\"] for cat in final_categories])\n",
    "MetadataCatalog.get(\"colpo_val\").set(thing_classes=[cat[\"name\"] for cat in final_categories])\n",
    "\n",
    "print(\"‚úÖ Datasets updated and re-registered: colpo_train and colpo_val.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340516fc-90cf-424d-a086-58ac6de58289",
   "metadata": {},
   "source": [
    "## Visualize Random Samples from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8e15c-820b-45d4-8cca-6b7cf9c37c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "def visualizar_muestras(dataset_dicts, metadata, num_muestras=5, titulo=\"Samples\"):\n",
    "    plt.figure(figsize=(15, 5 * num_muestras))\n",
    "    count = 0\n",
    "    for d in random.sample(dataset_dicts, min(num_muestras, len(dataset_dicts))):\n",
    "        img_path = d[\"file_name\"]\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"‚ùå Image not found: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
    "        vis = visualizer.draw_dataset_dict(d)\n",
    "        plt.subplot(num_muestras, 1, count + 1)\n",
    "        plt.imshow(vis.get_image()[:, :, ::-1])\n",
    "        plt.title(f\"{titulo} #{count+1}\")\n",
    "        plt.axis('off')\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"‚ö†Ô∏è Samples could not be visualized (all failed).\")\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f8335-281b-451a-b868-a05c5424b8b8",
   "metadata": {},
   "source": [
    "## Directory and Image Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753c044-ce86-4740-a2be-4b418f2efe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"üìÅ INTEL_DIR:\", INTEL_DIR)\n",
    "print(\"üñºÔ∏è Total images:\", len(list(INTEL_DIR.glob(\"*.jpg\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b67ef-3028-47e3-9a66-6f7567fd970d",
   "metadata": {},
   "source": [
    "## Execute Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e2e4e-04d2-4948-b1d3-f9d3c383f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute visualization\n",
    "visualizar_muestras(\n",
    "    DatasetCatalog.get(\"colpo_train\"),\n",
    "    MetadataCatalog.get(\"colpo_train\"),\n",
    "    num_muestras=5,\n",
    "    titulo=\"Visualization - Full Dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a2534-cc66-4d01-8066-19ac094976e7",
   "metadata": {},
   "source": [
    "## Display Random Examples with Real Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428dbe5-e1ad-4113-aa4a-05a700dbe739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "print(\"üñºÔ∏è Displaying 3 random examples with real segmentations...\")\n",
    "\n",
    "for d in random.sample(get_colpo_dicts_train(), 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    v = Visualizer(\n",
    "        img[:, :, ::-1],\n",
    "        metadata=MetadataCatalog.get(\"colpo_train\"),\n",
    "        scale=0.5,\n",
    "        instance_mode=ColorMode.IMAGE_BW  # Grayscale background for emphasis\n",
    "    )\n",
    "    v = v.draw_dataset_dict(d)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.title(d[\"file_name\"])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327b1c1-ef6f-4faa-b505-1adbe46ba544",
   "metadata": {},
   "source": [
    "## Free Memory (Optional but Useful if Coming from a Previous Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099832ba-7386-4a97-aca6-934ef3a5a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_with_max_free_memory():\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        free_memories = [int(x) for x in result.strip().split('\\n')]\n",
    "        best_gpu = free_memories.index(max(free_memories))\n",
    "        print(f\"üß† Selecting GPU {best_gpu} with {free_memories[best_gpu]} MiB free.\")\n",
    "        return best_gpu\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while querying GPU: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Set the visible GPU\n",
    "best_gpu_id = get_gpu_with_max_free_memory()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(best_gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bddfe3-7f42-469c-83e7-2dc37c06b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# üîÅ Remove datasets if already registered\n",
    "for dname in [\"colpo_train\", \"colpo_val\"]:\n",
    "    if dname in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(dname)\n",
    "\n",
    "# ‚úÖ Re-register\n",
    "DatasetCatalog.register(\"colpo_train\", get_colpo_dicts_train)\n",
    "DatasetCatalog.register(\"colpo_val\", get_colpo_dicts_val)\n",
    "\n",
    "MetadataCatalog.get(\"colpo_train\").set(thing_classes=[c[\"name\"] for c in final_categories])\n",
    "MetadataCatalog.get(\"colpo_val\").set(thing_classes=[c[\"name\"] for c in final_categories])\n",
    "\n",
    "print(\"‚úÖ Datasets registered: colpo_train and colpo_val.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5ca38-f9af-4f24-b44f-488b67b678d9",
   "metadata": {},
   "source": [
    "## Training Configuration and Model Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979c812-a7e2-4031-9301-df0b0747dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Output directory for trained models\n",
    "OUTPUT_DIR = \"output/colpo_model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ‚öôÔ∏è Model configuration\n",
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "# üß† Use base model architecture\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# üì¶ Load only backbone weights (without pretrained head)\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "# üìä Custom datasets\n",
    "cfg.DATASETS.TRAIN = (\"colpo_train\",)\n",
    "cfg.DATASETS.TEST = (\"colpo_val\",)\n",
    "\n",
    "# üéØ Custom classes\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
    "cfg.MODEL.ROI_HEADS.NAME = \"StandardROIHeads\"  # üëà Avoid conflict with COCO head\n",
    "\n",
    "# üß™ Score threshold for predictions\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "\n",
    "# üöÄ Device: GPU\n",
    "cfg.MODEL.DEVICE = \"cuda:0\"\n",
    "\n",
    "# üßÆ Optimizer hyperparameters\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 3000  # üëà Increased to ensure actual learning\n",
    "cfg.SOLVER.STEPS = []  # No learning rate decay\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "\n",
    "# üß™ Evaluation frequency\n",
    "cfg.TEST.EVAL_PERIOD = 500\n",
    "\n",
    "# ‚úÖ Train from scratch (no resume)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ce973-f96c-4798-a5af-becc13d31d53",
   "metadata": {},
   "source": [
    "## CONFIGURATION WITH TRAINED CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650799-d0b2-4ce4-9220-ac207d490d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = \"output/colpo_model\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.DATASETS.TEST = (\"colpo_val\",)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "cfg.MODEL.DEVICE = \"cuda:0\"\n",
    "\n",
    "# Register classes\n",
    "MetadataCatalog.get(\"colpo_val\").thing_classes = [\"aw\", \"m\", \"p\", \"av\", \"y\"]\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a99fa-de06-4dce-a595-b860be140e9e",
   "metadata": {},
   "source": [
    "## EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcd58b-d8a3-4ae3-b0e6-7f5edf8a55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = COCOEvaluator(\"colpo_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"colpo_val\")\n",
    "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "print(\"\\nüìä Evaluation results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5792f3-b94a-48c6-b299-64db3e583829",
   "metadata": {},
   "source": [
    "##  CLASS-WISE AP (Manual Computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdb1cb-af1d-4338-b2d4-13e5e0ec5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_JSON_PATH = Path(\"output/colpo_model/colpo_val_coco_format.json\")\n",
    "coco_gt = COCO(str(COCO_JSON_PATH))\n",
    "coco_dt = coco_gt.loadRes(\"output/colpo_model/coco_instances_results.json\")\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "cats = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "class_names = [c[\"name\"] for c in cats]\n",
    "ap_results = []\n",
    "for idx, name in enumerate(class_names):\n",
    "    precision = coco_eval.eval[\"precision\"][:, :, idx, 0, 0]\n",
    "    precision = precision[precision > -1]\n",
    "    ap = np.mean(precision) if precision.size else float(\"nan\")\n",
    "    ap_results.append((name, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b6924-8a02-46ed-b8a0-4f5a1ae7f5b7",
   "metadata": {},
   "source": [
    "## HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16b461-ba85-42d2-b5c5-81c60bea1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap = pd.DataFrame(ap_results, columns=[\"Class\", \"AP\"]).dropna()\n",
    "sns.heatmap(df_ap.set_index(\"Class\").T, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"üìä AP per Class (manually calculated)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d477a1-ec25-4d9c-a3d5-9159ec4a1fde",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315654c7-3067-4a97-9d1a-d2b7f8b6c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = DatasetCatalog.get(\"colpo_val\")\n",
    "for d in random.sample(dataset_val, 5):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   MetadataCatalog.get(\"colpo_val\"),\n",
    "                   scale=0.5,\n",
    "                   instance_mode=ColorMode.IMAGE_BW)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2.imshow(\"Prediction\", out.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3530a-ef42-47af-bcef-dca830c9e72b",
   "metadata": {},
   "source": [
    "## CONFIGURATION REVIEW FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3cc7e-125d-4f7e-a957-789f3888f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_configuracion(cfg):\n",
    "    print(\"\\nüß† CONFIGURATION REVIEW\")\n",
    "    print(\"üì¶ Weights loaded from:\", cfg.MODEL.WEIGHTS)\n",
    "    print(\"üìä Number of classes:\", cfg.MODEL.ROI_HEADS.NUM_CLASSES)\n",
    "    print(\"üìÅ VAL dataset:\", cfg.DATASETS.TEST)\n",
    "    print(\"üéØ Score threshold:\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
    "    print(\"üìé Registered categories:\", MetadataCatalog.get(cfg.DATASETS.TEST[0]).thing_classes)\n",
    "    if cfg.MODEL.ROI_HEADS.NUM_CLASSES != len(MetadataCatalog.get(cfg.DATASETS.TEST[0]).thing_classes):\n",
    "        print(\"‚ùå MISMATCH between NUM_CLASSES and registered categories. Check metadata registration.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Number of classes and categories are aligned.\")\n",
    "\n",
    "revisar_configuracion(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00beb9-f266-43f4-9796-475d756d1f8c",
   "metadata": {},
   "source": [
    "## Generate a consistent mapping of image names to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3d85a-a978-4a55-ba4d-ae37dea9e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Paths\n",
    "COCO_JSON_PATH = Path(\"output/colpo_model/colpo_val_coco_format.json\")\n",
    "VAL_IMG_DIR = Path(\"/tf/workspace/ColpoToolLab/intel\")\n",
    "\n",
    "# üîÅ Build a consistent map between image names and IDs\n",
    "# ‚ö†Ô∏è Reuse this also when constructing the COCO JSON annotation section\n",
    "print(\"\\nüîß Generating img_id_map from unique names in the 'colpo_val' dataset...\")\n",
    "dataset_val = DatasetCatalog.get(\"colpo_val\")\n",
    "\n",
    "nombres_imagenes = sorted(list(set([Path(d[\"file_name\"]).name for d in dataset_val])))\n",
    "img_id_map = {nombre: i for i, nombre in enumerate(nombres_imagenes)}\n",
    "\n",
    "# Show a sample of the generated map\n",
    "print(\"üó∫Ô∏è Sample of img_id_map (first 5):\")\n",
    "for k in list(img_id_map.keys())[:5]:\n",
    "    print(f\"  {k} ‚Üí ID {img_id_map[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa430ec-7891-4b28-9685-dde856c991dd",
   "metadata": {},
   "source": [
    "## Check the current IDs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59766651-8413-4cdb-a6a7-b10a2849cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Check current IDs in dataset_val\n",
    "print(f\"\\nüîç Total elements in 'colpo_val': {len(dataset_val)}\")\n",
    "print(\"\\nüì∑ Examples from the 'colpo_val' dataset:\")\n",
    "for i, d in enumerate(dataset_val[:3]):\n",
    "    nombre = Path(d[\"file_name\"]).name\n",
    "    id_expected = img_id_map.get(nombre, \"‚ùå Not found\")\n",
    "    print(f\"  [{i}] Current ID: {d['image_id']} | Expected: {id_expected} | File: {nombre}\")\n",
    "    for ann in d[\"annotations\"]:\n",
    "        print(f\"    - Class: {ann['category_id']} | Segmentation: {str(ann['segmentation'])[:30]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29e0db-04b5-43ef-8189-b28dec122725",
   "metadata": {},
   "source": [
    "## Check that image IDs are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7824644-3281-4949-9ab2-f4d0cfda8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 2: Check that image IDs are unique in the dataset\n",
    "ids_dataset = [d[\"image_id\"] for d in dataset_val]\n",
    "if len(set(ids_dataset)) == len(ids_dataset):\n",
    "    print(\"\\n‚úÖ All 'image_id' values in 'colpo_val' are unique.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Duplicate 'image_id' values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe15ae-1e25-4b59-8518-ce7f5854b190",
   "metadata": {},
   "source": [
    "## Load the exported COCO JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fbb0c-7e36-4929-9f92-bea18ba2aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 3: Load the exported JSON file (if it exists)\n",
    "if COCO_JSON_PATH.exists():\n",
    "    print(f\"\\nüìÇ File found: {COCO_JSON_PATH.name}\")\n",
    "    with open(COCO_JSON_PATH, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    ids_json = [img[\"id\"] for img in coco_data.get(\"images\", [])]\n",
    "    print(f\"üìä Total images in COCO JSON: {len(ids_json)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c83475-1ebf-4e1b-97fd-75c036fce66a",
   "metadata": {},
   "source": [
    "## Compare IDs between dataset and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64de49-21f3-44f7-8adb-01d5031f6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ‚úÖ Step 4: Compare image IDs between dataset and exported JSON\n",
    "    print(\"\\nüîç Comparing 'image_id' between dataset and exported JSON...\")\n",
    "    ids_dataset_set = set(ids_dataset)\n",
    "    ids_json_set = set(ids_json)\n",
    "\n",
    "    missing_in_json = ids_dataset_set - ids_json_set\n",
    "    missing_in_dataset = ids_json_set - ids_dataset_set\n",
    "\n",
    "    if missing_in_json:\n",
    "        print(f\"‚ö†Ô∏è IDs present in the dataset but missing in the JSON: {sorted(missing_in_json)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All dataset IDs are present in the JSON.\")\n",
    "\n",
    "    if missing_in_dataset:\n",
    "        print(f\"‚ö†Ô∏è IDs present in the JSON but missing in the dataset: {sorted(missing_in_dataset)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All JSON IDs are present in the dataset.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå File not found: {COCO_JSON_PATH}\")\n",
    "    print(\"   ‚Üí Make sure it was properly generated during evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fd810-7042-4982-841e-ad867948419c",
   "metadata": {},
   "source": [
    "## Regenerate the COCO JSON file with consistent image IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b8ac1-bd11-4d21-b0d2-621cccc8288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ If IDs do not match, regenerate the COCO JSON file with consistent IDs\n",
    "print(\"\\nüîÅ Regenerating COCO JSON file with consistent IDs...\")\n",
    "\n",
    "# You should already have this list loaded (anotaciones_val)\n",
    "# If not available in this cell, load it from where it was previously generated\n",
    "# anotaciones_val = [...]\n",
    "\n",
    "json_coco_val = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"aw\"},\n",
    "        {\"id\": 2, \"name\": \"m\"},\n",
    "        {\"id\": 3, \"name\": \"p\"},\n",
    "        {\"id\": 4, \"name\": \"av\"},\n",
    "        {\"id\": 5, \"name\": \"y\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "import cv2\n",
    "ann_id_counter = 1\n",
    "for nombre, img_id in img_id_map.items():\n",
    "    ruta = VAL_IMG_DIR / nombre\n",
    "    if not ruta.exists():\n",
    "        print(f\"‚ùå Image not found: {nombre}\")\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(str(ruta))\n",
    "    if img is None:\n",
    "        print(f\"‚ö†Ô∏è Unable to read image: {nombre}\")\n",
    "        continue\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    json_coco_val[\"images\"].append({\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": nombre,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    for ann in [a for a in anotaciones_val if a[\"nombre\"] == nombre]:\n",
    "        segmentacion = ann[\"segmentacion\"]\n",
    "        categoria_id = ann[\"categoria_id\"]\n",
    "\n",
    "        x_coords = segmentacion[0][0::2]\n",
    "        y_coords = segmentacion[0][1::2]\n",
    "        bbox = [\n",
    "            float(min(x_coords)),\n",
    "            float(min(y_coords)),\n",
    "            float(max(x_coords) - min(x_coords)),\n",
    "            float(max(y_coords) - min(y_coords))\n",
    "        ]\n",
    "        area = bbox[2] * bbox[3]\n",
    "\n",
    "        json_coco_val[\"annotations\"].append({\n",
    "            \"id\": ann_id_counter,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": categoria_id,\n",
    "            \"segmentation\": segmentacion,\n",
    "            \"bbox\": bbox,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        ann_id_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e26a0f-ecf5-4d1d-b4ad-79e36a6f1f61",
   "metadata": {},
   "source": [
    "## Save the updated COCO JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024d513-9cfe-4097-8826-9208c1413d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated JSON\n",
    "with open(COCO_JSON_PATH, \"w\") as f:\n",
    "    json.dump(json_coco_val, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ New COCO JSON file saved at: {COCO_JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f21f94-6c41-430a-9f89-9499b0856173",
   "metadata": {},
   "source": [
    "## Perform inference and evaluation using COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9fbc5-a00a-4d2b-902e-6c6f29c2f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# ‚öôÔ∏è Create COCO evaluator\n",
    "evaluator = COCOEvaluator(\"colpo_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"colpo_val\")\n",
    "\n",
    "# üß† Run inference and evaluation\n",
    "results = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "\n",
    "# üìä Display results\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dc0a1-5075-4fc3-9e07-5c418dff38d8",
   "metadata": {},
   "source": [
    "## Evaluate segmentation performance using COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de44b4a-82c6-4431-b7eb-62db63199b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation.coco_evaluation import instances_to_coco_json\n",
    "from detectron2.data.datasets.coco import load_coco_json\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# ‚ö†Ô∏è Only if COCOEvaluator was used\n",
    "coco_gt = COCO(str(COCO_JSON_PATH))\n",
    "coco_dt = coco_gt.loadRes(\"output/colpo_model/coco_instances_results.json\")\n",
    "\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85b943-a89d-4880-a7b9-4329bc182a88",
   "metadata": {},
   "source": [
    "## Calculate and display average precision per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdef600-759d-4ca0-949e-214dd433ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AP per class\n",
    "cats = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "class_names = [c[\"name\"] for c in cats]\n",
    "ap_per_class = coco_eval.eval[\"precision\"]  # [T, R, K, A, M]\n",
    "import numpy as np\n",
    "\n",
    "ap_results = []\n",
    "for idx, name in enumerate(class_names):\n",
    "    precision = coco_eval.eval[\"precision\"][:, :, idx, 0, 0]\n",
    "    precision = precision[precision > -1]\n",
    "    ap = np.mean(precision) if precision.size else float(\"nan\")\n",
    "    ap_results.append((name, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cf809-a2de-4660-875d-80499a6c4279",
   "metadata": {},
   "source": [
    "## Visualize AP values as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baea5e-e4ef-49cd-87e1-26d319ba2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AP values as a table\n",
    "df_ap = pd.DataFrame(ap_results, columns=[\"Class\", \"AP\"]).dropna()\n",
    "sns.heatmap(df_ap.set_index(\"Class\").T, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"üìä AP per class (manually calculated)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ae7ca-5875-41aa-8faa-8a15ab82c1be",
   "metadata": {},
   "source": [
    "## Visualize random prediction examples from the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e15d0-bff0-40a2-90ea-9a98e6018675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Test some visual predictions\n",
    "for d in random.sample(dataset_val, 5):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(\n",
    "        im[:, :, ::-1],\n",
    "        MetadataCatalog.get(\"colpo_val\"),\n",
    "        scale=0.5,\n",
    "        instance_mode=ColorMode.IMAGE_BW  # or use ColorMode.SEGMENTATION\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2.imshow(\"Prediction\", out.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58e737-8d36-4ffd-bd05-14842cab91c3",
   "metadata": {},
   "source": [
    "## Review the model configuration and metadata consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233d3cd-bd3b-4feb-8de9-2f3a3bfd7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def revisar_configuracion(cfg):\n",
    "    print(\"\\nüß† CONFIGURATION REVIEW\")\n",
    "    print(\"üì¶ Weights loaded from:\", cfg.MODEL.WEIGHTS)\n",
    "    print(\"üìä Number of classes:\", cfg.MODEL.ROI_HEADS.NUM_CLASSES)\n",
    "    print(\"üìÅ VAL dataset:\", cfg.DATASETS.TEST)\n",
    "    print(\"üéØ Score threshold:\", cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
    "    print(\"üìé Registered categories:\", MetadataCatalog.get(cfg.DATASETS.TEST[0]).thing_classes)\n",
    "    \n",
    "    # Useful validations\n",
    "    if cfg.MODEL.ROI_HEADS.NUM_CLASSES != len(MetadataCatalog.get(cfg.DATASETS.TEST[0]).thing_classes):\n",
    "        print(\"‚ùå MISMATCH between NUM_CLASSES and registered categories. Please verify metadata registration.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Number of classes and registered categories are aligned.\")\n",
    "\n",
    "# Run configuration review\n",
    "revisar_configuracion(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dd592-ddc0-40bc-bbfc-b3f02b5c05e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df723c-3bdd-4f03-8fb4-ddbdec4177f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98611b7-2897-4e04-b723-1b8791d68bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
